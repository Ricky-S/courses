{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting Exercise\n",
    "\n",
    "Many deep learning models run the danger of overfitting on the training set. When this happens, the model fails to generalize its performance to unseen data, such as a separate validation set. Here we present a simple exercise on how to recognize overfitting using our visualization tools, and how to apply Dropout layers to prevent overfitting.\n",
    "\n",
    "We use a simple network of convolutional layers on the CIFAR-10 dataset, a dataset of images belonging to 10 categories. \n",
    "\n",
    "The code below will build the model and train on the CIFAR-10 dataset for 25 epochs (~2 minutes on Titan X GPUs), displaying both the training cost as well as the cost on the validation set.\n",
    "\n",
    "Note: We highly recommend users run this model on Maxwell GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DISPLAY:neon:Downloading file: data/cifar10_batches/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Download Progress |██████████████████████████████████████████████████| "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DISPLAY:neon:Download Complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/backends/nervanacpu.py:681: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  array_output[numpy_ind.tolist()] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |                    |    6/391  batches, 2.77 cost, 86.68s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7422df70a0f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_gdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Misclassification error = %.1f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMisclassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/models/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, cost, optimizer, num_epochs, callbacks)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epoch_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/models/model.py\u001b[0m in \u001b[0;36m_epoch_fit\u001b[0;34m(self, dataset, callbacks)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_to_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/models/model.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, delta)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDeltas\u001b[0m \u001b[0mto\u001b[0m \u001b[0mpropagate\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \"\"\"\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/layers/container.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, error, alpha, beta)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;31m# for not-mkl op, deltas is cpu tensor, but it is shared thus may have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/layers/layer.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \"\"\"\n\u001b[1;32m   1528\u001b[0m         self.be.bprop_transform(self.nglayer, self.transform, self.outputs,\n\u001b[0;32m-> 1529\u001b[0;31m                                 error, self.deltas, type(self.transform) == Rectlin)\n\u001b[0m\u001b[1;32m   1530\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/backends/nervanamkl.py\u001b[0m in \u001b[0;36mbprop_transform\u001b[0;34m(self, nglayer, transform, outputs, error, deltas, relu)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbprop_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnglayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnglayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             super(NervanaMKL, self).bprop_transform(\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/transforms/activation.py\u001b[0m in \u001b[0;36mbprop\u001b[0;34m(self, x, nglayer, error, deltas)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mTensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0moptree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDerivative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \"\"\"\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbprop_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnglayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/backends/nervanamkl.py\u001b[0m in \u001b[0;36mbprop_relu\u001b[0;34m(self, layer, x, error, deltas, slope)\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_mkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mprimitives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_longlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnnPrimitives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmklEngine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRelu_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitOk_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitOk_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mdeltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_mkl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/nervananeon-2.6.0-py3.6.egg/neon/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_sigint_catch\u001b[0;34m(self, epoch, minibatch)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Checkpoint file saved to {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from neon.initializers import Gaussian\n",
    "from neon.optimizers import GradientDescentMomentum, Schedule\n",
    "from neon.layers import Conv, Dropout, Activation, Pooling, GeneralizedCost\n",
    "from neon.transforms import Rectlin, Softmax, CrossEntropyMulti, Misclassification\n",
    "from neon.models import Model\n",
    "from neon.data import CIFAR10\n",
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(batch_size=128, backend='mkl')\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 0.05\n",
    "weight_decay = 0.001\n",
    "num_epochs = 25\n",
    "data_dir = 'data/cifar10_batches'\n",
    "\n",
    "print(\"Loading Data\")\n",
    "dataset = CIFAR10(path=data_dir,\n",
    "                  normalize=True,\n",
    "                  contrast_normalize=False,\n",
    "                  whiten=False, pad_classes=True)\n",
    "train_set = dataset.train_iter\n",
    "valid_set = dataset.valid_iter\n",
    "\n",
    "print(\"Building Model\")\n",
    "init_uni = Gaussian(scale=0.05)\n",
    "opt_gdm = GradientDescentMomentum(learning_rate=float(learning_rate), momentum_coef=0.9,\n",
    "                                  wdecay=float(weight_decay),\n",
    "                                  schedule=Schedule(step_config=[200, 250, 300], change=0.1))\n",
    "\n",
    "relu = Rectlin()\n",
    "conv = dict(init=init_uni, batch_norm=False, activation=relu)\n",
    "convp1 = dict(init=init_uni, batch_norm=False, activation=relu, padding=1)\n",
    "convp1s2 = dict(init=init_uni, batch_norm=False, activation=relu, padding=1, strides=2)\n",
    "\n",
    "layers = [\n",
    "          Conv((3, 3, 64), **convp1),\n",
    "          Conv((3, 3, 64), **convp1s2),\n",
    "          Conv((3, 3, 128), **convp1),\n",
    "          Conv((3, 3, 128), **convp1s2),\n",
    "          Conv((3, 3, 128), **convp1),\n",
    "          Conv((1, 1, 128), **conv),\n",
    "          Conv((1, 1, 16), **conv),\n",
    "          Pooling(8, op=\"avg\"),\n",
    "          Activation(Softmax())]\n",
    "\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "mlp = Model(layers=layers)\n",
    "\n",
    "\n",
    "# configure callbacks\n",
    "callbacks = Callbacks(mlp, output_file='data.h5', eval_set=valid_set, eval_freq=1)\n",
    "\n",
    "print(\"Training\")\n",
    "mlp.fit(train_set, optimizer=opt_gdm, num_epochs=num_epochs, cost=cost, callbacks=callbacks)\n",
    "\n",
    "print('Misclassification error = %.1f%%' % (mlp.eval(valid_set, metric=Misclassification())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "You should notice that in the logs above, after around Epoch 15, the model begins to overfit. Even though the cost on the training set continues to decrease, the validation loss flattens (even increasing slightly). We can visualize these effects using the code below.\n",
    "\n",
    "Note: The same plots can be created using our `nvis` command line utility (see: http://neon.nervanasys.com/docs/latest/tools.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neon.visualizations.figure import cost_fig, hist_fig, deconv_summary_page\n",
    "from neon.visualizations.data import h5_cost_data, h5_hist_data, h5_deconv_data\n",
    "from bokeh.plotting import output_notebook, show\n",
    "\n",
    "cost_data = h5_cost_data('data.h5', False)\n",
    "output_notebook()\n",
    "show(cost_fig(cost_data, 400, 800, epoch_axis=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This situation illustrates the importance of plotting the validation loss (blue) in addition to the training cost (red). The training cost may mislead the user into thinking that model is continuing to perform well, but we can see from the validation loss that the model has begun to overfit.\n",
    "\n",
    "## Dropout layers\n",
    "\n",
    "To correct overfitting, we introduce `Dropout` layers to the model, as shown below. `Dropout` layers randomly silence a subset of units for each minibatch, and are an effective means of preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "          Conv((3, 3, 64), **convp1),\n",
    "          Conv((3, 3, 64), **convp1s2),\n",
    "          ... # call Dropout() and set the parameter 'keep' to be the fraction of inputs that should be stochastically kept\n",
    "          Conv((3, 3, 128), **convp1),\n",
    "          Conv((3, 3, 128), **convp1s2),\n",
    "          ... # add dropout\n",
    "          Conv((3, 3, 128), **convp1),\n",
    "          Conv((1, 1, 128), **conv),\n",
    "          Conv((1, 1, 16), **conv),\n",
    "          Pooling(8, op=\"avg\"),\n",
    "          Activation(Softmax())]\n",
    "\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti())\n",
    "\n",
    "mlp = Model(layers=layers)\n",
    "\n",
    "\n",
    "# configure callbacks\n",
    "callbacks = Callbacks(mlp, output_file='data.h5', eval_set=valid_set, eval_freq=1)\n",
    "\n",
    "print(\"Training\")\n",
    "mlp.fit(train_set, optimizer=opt_gdm, num_epochs=num_epochs, cost=cost, callbacks=callbacks)\n",
    "\n",
    "print('Misclassification error = %.1f%%' % (mlp.eval(valid_set, metric=Misclassification())*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then plot the results of the training run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neon.visualizations.figure import cost_fig, hist_fig, deconv_summary_page\n",
    "from neon.visualizations.data import h5_cost_data, h5_hist_data, h5_deconv_data\n",
    "from bokeh.plotting import output_notebook, show\n",
    "\n",
    "cost_data = h5_cost_data('data.h5', False)\n",
    "output_notebook()\n",
    "show(cost_fig(cost_data, 400, 800, epoch_axis=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dropout layers in place, the model is now able to continue performing well on the validation set beyond epoch 15. The validation loss (blue) is not shifted downwards compared to the previous figure, and the model reaches better validation performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
